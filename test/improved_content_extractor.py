#!/usr/bin/env python3
"""
üîç –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê
–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
"""

import asyncio
import aiohttp
import re
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from urllib.parse import urlparse
import trafilatura
from newspaper import Article
from bs4 import BeautifulSoup
import readability
import justext
import nltk
from nltk.tokenize import sent_tokenize
import hashlib

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –û–¢–ö–õ–Æ–ß–ê–ï–ú DEBUG –õ–û–ì–ò –û–¢ –°–¢–û–†–û–ù–ù–ò–• –ë–ò–ë–õ–ò–û–¢–ï–ö
try:
    from internet_intelligence_logger import disable_third_party_debug_logs
    disable_third_party_debug_logs()
except ImportError:
    # Fallback –µ—Å–ª–∏ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
    third_party_loggers = ['htmldate', 'trafilatura', 'newspaper', 'readability', 'justext',
                          'bs4', 'urllib3', 'aiohttp', 'asyncio', 'charset_normalizer',
                          'requests', 'feedparser', 'nltk', 'lxml', 'html5lib']
    for logger_name in third_party_loggers:
        logging.getLogger(logger_name).setLevel(logging.WARNING)

@dataclass
class ExtractedContent:
    """–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    text: str
    title: str
    author: Optional[str]
    publish_date: Optional[str]
    language: str
    word_count: int
    relevance_score: float
    extraction_method: str
    url: str
    metadata: Dict[str, Any]

class ImprovedContentExtractor:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.session = None
        self.user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.min_word_count = 50
        self.max_word_count = 10000
        self.min_relevance_score = 0.3
        
        # –°–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.problematic_domains = {
            'wikipedia.org': 'wikipedia',
            'youtube.com': 'video',
            'facebook.com': 'social',
            'twitter.com': 'social',
            'instagram.com': 'social',
            'linkedin.com': 'social'
        }
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=15),
                headers={'User-Agent': self.user_agent}
            )
        return self.session
    
    def _get_domain_type(self, url: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–º–µ–Ω–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        domain = urlparse(url).netloc.lower()
        
        for problematic_domain, domain_type in self.problematic_domains.items():
            if problematic_domain in domain:
                return domain_type
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –¥–æ–º–µ–Ω–µ
        if any(word in domain for word in ['news', 'rbc', 'ria', 'tass', 'interfax']):
            return 'news'
        elif any(word in domain for word in ['blog', 'medium', 'substack']):
            return 'blog'
        elif any(word in domain for word in ['forum', 'reddit', 'stackoverflow']):
            return 'forum'
        else:
            return 'general'
    
    async def extract_with_trafilatura(self, html: str, url: str) -> Optional[ExtractedContent]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é Trafilatura"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Trafilatura
            configs = [
                {'include_formatting': True, 'include_links': False},
                {'include_formatting': False, 'include_links': False},
                {'include_formatting': True, 'include_links': True}
            ]
            
            for config in configs:
                extracted = trafilatura.extract(html, **config)
                if extracted and len(extracted.strip()) > 100:
                    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    metadata = trafilatura.extract_metadata(html)
                    
                    return ExtractedContent(
                        text=extracted.strip(),
                        title=metadata.title if metadata else '',
                        author=metadata.author if metadata else None,
                        publish_date=metadata.date if metadata else None,
                        language=metadata.language if metadata else 'ru',
                        word_count=len(extracted.split()),
                        relevance_score=0.0,  # –ë—É–¥–µ—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ –ø–æ–∑–∂–µ
                        extraction_method='trafilatura',
                        url=url,
                        metadata={'config': config}
                    )
            
            return None
        except Exception as e:
            logger.debug(f"Trafilatura failed for {url}: {e}")
            return None
    
    async def extract_with_newspaper(self, html: str, url: str) -> Optional[ExtractedContent]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é Newspaper3k"""
        try:
            article = Article(url)
            article.download(input_html=html)
            article.parse()
            
            if article.text and len(article.text.strip()) > 100:
                return ExtractedContent(
                    text=article.text.strip(),
                    title=article.title or '',
                    author=article.authors[0] if article.authors else None,
                    publish_date=article.publish_date.isoformat() if article.publish_date else None,
                    language=article.language or 'ru',
                    word_count=len(article.text.split()),
                    relevance_score=0.0,
                    extraction_method='newspaper',
                    url=url,
                    metadata={'authors': article.authors, 'top_image': article.top_image}
                )
            
            return None
        except Exception as e:
            logger.debug(f"Newspaper failed for {url}: {e}")
            return None
    
    async def extract_with_readability(self, html: str, url: str) -> Optional[ExtractedContent]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é Readability"""
        try:
            doc = readability.Document(html)
            if doc.title() and doc.summary():
                return ExtractedContent(
                    text=doc.summary(),
                    title=doc.title(),
                    author=None,
                    publish_date=None,
                    language='ru',
                    word_count=len(doc.summary().split()),
                    relevance_score=0.0,
                    extraction_method='readability',
                    url=url,
                    metadata={'short_title': doc.short_title()}
                )
            
            return None
        except Exception as e:
            logger.debug(f"Readability failed for {url}: {e}")
            return None
    
    async def extract_with_justext(self, html: str, url: str) -> Optional[ExtractedContent]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é Justext"""
        try:
            paragraphs = justext.justext(html, justext.get_stoplist("Russian"))
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ö–æ—Ä–æ—à–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            good_texts = []
            for paragraph in paragraphs:
                if not paragraph.is_boilerplate and paragraph.text.strip():
                    good_texts.append(paragraph.text.strip())
            
            if good_texts:
                full_text = '\n\n'.join(good_texts)
                if len(full_text) > 100:
                    return ExtractedContent(
                        text=full_text,
                        title='',
                        author=None,
                        publish_date=None,
                        language='ru',
                        word_count=len(full_text.split()),
                        relevance_score=0.0,
                        extraction_method='justext',
                        url=url,
                        metadata={'paragraphs_count': len(good_texts)}
                    )
            
            return None
        except Exception as e:
            logger.debug(f"Justext failed for {url}: {e}")
            return None
    
    async def extract_with_bs4(self, html: str, url: str) -> Optional[ExtractedContent]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é BeautifulSoup (fallback)"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
                element.decompose()
            
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content_selectors = [
                'article',
                '[role="main"]',
                '.content',
                '.post-content',
                '.article-content',
                '.entry-content',
                '#content',
                '.main-content'
            ]
            
            content = None
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content and len(content.get_text().strip()) > 100:
                    break
            
            if not content:
                # –ë–µ—Ä–µ–º body –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
                content = soup.find('body')
            
            if content:
                text = content.get_text(separator='\n', strip=True)
                # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
                text = re.sub(r'\n\s*\n', '\n\n', text)
                text = re.sub(r'\s+', ' ', text)
                
                if len(text) > 100:
                    return ExtractedContent(
                        text=text,
                        title=soup.find('title').get_text() if soup.find('title') else '',
                        author=None,
                        publish_date=None,
                        language='ru',
                        word_count=len(text.split()),
                        relevance_score=0.0,
                        extraction_method='beautifulsoup',
                        url=url,
                        metadata={'used_selector': selector if content else 'body'}
                    )
            
            return None
        except Exception as e:
            logger.debug(f"BeautifulSoup failed for {url}: {e}")
            return None
    
    def calculate_relevance_score(self, content: ExtractedContent, query: str) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫ –∑–∞–ø—Ä–æ—Å—É"""
        query_words = set(query.lower().split())
        content_words = set(content.text.lower().split())
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Å–ª–æ–≤
        matching_words = len(query_words.intersection(content_words))
        
        # –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É)
        length_score = 1.0
        if content.word_count < 100:
            length_score = 0.3
        elif content.word_count > 5000:
            length_score = 0.7
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º trafilatura –∏ newspaper)
        method_scores = {
            'trafilatura': 1.0,
            'newspaper': 0.9,
            'readability': 0.8,
            'justext': 0.7,
            'beautifulsoup': 0.5
        }
        method_score = method_scores.get(content.extraction_method, 0.5)
        
        # –ù–∞–ª–∏—á–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_score = 1.0 if content.title else 0.8
        
        # –û–±—â–∏–π —Å–∫–æ—Ä
        relevance = (
            (matching_words / max(len(query_words), 1)) * 0.4 +
            length_score * 0.2 +
            method_score * 0.2 +
            title_score * 0.1 +
            (1.0 if content.author else 0.8) * 0.1
        )
        
        return min(relevance, 1.0)
    
    def filter_content(self, content: ExtractedContent, query: str) -> bool:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        if content.word_count < self.min_word_count:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        if content.word_count > self.max_word_count:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        relevance = self.calculate_relevance_score(content, query)
        if relevance < self.min_relevance_score:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ø–∞–º/—Ä–µ–∫–ª–∞–º—É
        spam_indicators = [
            '–∫—É–ø–∏—Ç—å —Å–µ–π—á–∞—Å', '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞', '–±–µ—Å–ø–ª–∞—Ç–Ω–æ',
            '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å', '–ø–æ–¥–ø–∏—à–∏—Ç–µ—Å—å', '—Ä–µ–∫–ª–∞–º–∞'
        ]
        
        text_lower = content.text.lower()
        spam_count = sum(1 for indicator in spam_indicators if indicator in text_lower)
        if spam_count > 3:
            return False
        
        return True
    
    async def extract_content(self, url: str, query: str) -> Optional[ExtractedContent]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            session = await self._get_session()
            
            async with session.get(url, timeout=15) as response:
                if response.status != 200:
                    logger.warning(f"HTTP {response.status} for {url}")
                    return None
                
                html = await response.text()
                domain_type = self._get_domain_type(url)
                
                # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–æ–º–µ–Ω–∞
                extraction_methods = []
                
                if domain_type == 'news':
                    extraction_methods = [
                        self.extract_with_newspaper,
                        self.extract_with_trafilatura,
                        self.extract_with_readability
                    ]
                elif domain_type == 'blog':
                    extraction_methods = [
                        self.extract_with_trafilatura,
                        self.extract_with_newspaper,
                        self.extract_with_justext
                    ]
                elif domain_type == 'wikipedia':
                    extraction_methods = [
                        self.extract_with_trafilatura,
                        self.extract_with_bs4
                    ]
                else:
                    extraction_methods = [
                        self.extract_with_trafilatura,
                        self.extract_with_newspaper,
                        self.extract_with_readability,
                        self.extract_with_justext,
                        self.extract_with_bs4
                    ]
                
                # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã
                extracted_contents = []
                for method in extraction_methods:
                    try:
                        content = await method(html, url)
                        if content:
                            extracted_contents.append(content)
                    except Exception as e:
                        logger.debug(f"Method {method.__name__} failed: {e}")
                
                if not extracted_contents:
                    logger.warning(f"No content extracted from {url}")
                    return None
                
                # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                best_content = None
                best_score = 0.0
                
                for content in extracted_contents:
                    relevance = self.calculate_relevance_score(content, query)
                    content.relevance_score = relevance
                    
                    if relevance > best_score and self.filter_content(content, query):
                        best_score = relevance
                        best_content = content
                
                if best_content:
                    logger.info(f"‚úÖ Extracted {best_content.word_count} words from {url} (relevance: {best_score:.2f})")
                    return best_content
                else:
                    logger.warning(f"No suitable content found for {url}")
                    return None
                
        except Exception as e:
            logger.error(f"Error extracting content from {url}: {e}")
            return None
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session and not self.session.closed:
            await self.session.close()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
extractor = None

async def get_extractor() -> ImprovedContentExtractor:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
    global extractor
    if extractor is None:
        extractor = ImprovedContentExtractor()
    return extractor

async def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
    extractor = await get_extractor()
    
    test_urls = [
        "https://www.rbc.ru/technology_and_media/15/07/2025/65b123a29a794767a69a45a3",
        "https://ria.ru/20250715/novosti-1951234567.html",
        "https://www.interfax.ru/russia/123456"
    ]
    
    query = "–Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —Å–µ–≥–æ–¥–Ω—è"
    
    for url in test_urls:
        print(f"\nüîç Testing: {url}")
        content = await extractor.extract_content(url, query)
        
        if content:
            print(f"‚úÖ Extracted: {content.word_count} words")
            print(f"üìù Method: {content.extraction_method}")
            print(f"üéØ Relevance: {content.relevance_score:.2f}")
            print(f"üìÑ Preview: {content.text[:200]}...")
        else:
            print("‚ùå Failed to extract content")
    
    await extractor.close()

if __name__ == "__main__":
    asyncio.run(main()) 