#!/usr/bin/env python3
"""
üåê –ü–†–û–°–¢–ê–Ø –ò –†–ê–ë–û–ß–ê–Ø –°–ò–°–¢–ï–ú–ê –ò–ù–¢–ï–†–ù–ï–¢-–ò–ù–¢–ï–õ–õ–ï–ö–¢–ê
–ë–∞–∑–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
"""

import requests
import re
import logging
from typing import Dict, List, Optional
from urllib.parse import quote_plus, urlparse
from bs4 import BeautifulSoup
import time
import json
import random

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleInternetSystem:
    """–ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def clean_query(self, query: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        # –£–±–∏—Ä–∞–µ–º "–±–æ—Ç" –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        query = re.sub(r'\b–±–æ—Ç\b', '', query, flags=re.IGNORECASE)
        query = re.sub(r'\b–±–æ—Ç—É\b', '', query, flags=re.IGNORECASE)
        query = re.sub(r'\b–±–æ—Ç–æ–º\b', '', query, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ —Å–ª–æ–≤–∞
        query = re.sub(r'[^\w\s–∞-—è—ë-]', ' ', query, flags=re.IGNORECASE)
        query = re.sub(r'\s+', ' ', query).strip()
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
        if not query or len(query) < 3:
            query = "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"
        
        logger.info(f"–û—á–∏—â–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}'")
        return query
    
    def search_duckduckgo(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo API"""
        try:
            search_url = f"https://api.duckduckgo.com/?q={quote_plus(query)}&format=json&no_html=1&skip_disambig=1"
            
            logger.info(f"DuckDuckGo –ø–æ–∏—Å–∫: {query}")
            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if data.get('Abstract'):
                results.append({
                    'title': data.get('Heading', '–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞'),
                    'url': data.get('AbstractURL', ''),
                    'snippet': data.get('Abstract', ''),
                    'source': 'DuckDuckGo'
                })
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
            for topic in data.get('RelatedTopics', [])[:8]:
                if isinstance(topic, dict) and topic.get('Text'):
                    results.append({
                        'title': topic.get('Text', '')[:100],
                        'url': topic.get('FirstURL', ''),
                        'snippet': topic.get('Text', ''),
                        'source': 'DuckDuckGo'
                    })
            
            logger.info(f"DuckDuckGo –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ DuckDuckGo –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def search_bing(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Bing (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥)"""
        try:
            search_url = f"https://www.bing.com/search?q={quote_plus(query)}"
            
            logger.info(f"Bing –ø–æ–∏—Å–∫: {query}")
            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            
            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ Bing
            search_results = soup.find_all('li', class_='b_algo')
            
            for result in search_results[:8]:
                try:
                    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = result.find('h2')
                    if not title_elem:
                        continue
                    
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                    link_elem = title_elem.find('a')
                    if not link_elem:
                        continue
                    
                    # –ò—â–µ–º —Å–Ω–∏–ø–ø–µ—Ç
                    snippet_elem = result.find('p')
                    
                    title = title_elem.get_text().strip()
                    url = link_elem.get('href', '')
                    snippet = snippet_elem.get_text().strip() if snippet_elem else ''
                    
                    if url and not url.startswith('/'):
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'source': 'Bing'
                        })
                
                except Exception as e:
                    continue
            
            logger.info(f"Bing –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Bing –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def search_google(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google (—Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º)"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ User-Agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            ]
            
            self.session.headers['User-Agent'] = random.choice(user_agents)
            
            search_url = f"https://www.google.com/search?q={quote_plus(query)}&num=10"
            
            logger.info(f"Google –ø–æ–∏—Å–∫: {query}")
            response = self.session.get(search_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            
            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            search_results = soup.find_all('div', class_='g')
            
            for result in search_results[:8]:
                try:
                    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = result.find('h3')
                    if not title_elem:
                        continue
                    
                    # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                    link_elem = result.find('a')
                    if not link_elem:
                        continue
                    
                    # –ò—â–µ–º —Å–Ω–∏–ø–ø–µ—Ç
                    snippet_elem = result.find('div', class_='VwiC3b')
                    if not snippet_elem:
                        snippet_elem = result.find('span', class_='aCOpRe')
                    
                    title = title_elem.get_text().strip()
                    url = link_elem.get('href', '')
                    snippet = snippet_elem.get_text().strip() if snippet_elem else ''
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å—Å—ã–ª–∫–∞ Google
                    if url and not url.startswith('/') and 'google.com' not in url:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'source': 'Google'
                        })
                
                except Exception as e:
                    continue
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥
            if not results:
                logger.info("–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥")
                results = self._alternative_search_parsing(soup, query)
            
            logger.info(f"Google –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Google –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _alternative_search_parsing(self, soup: BeautifulSoup, query: str) -> List[Dict]:
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        results = []
        
        try:
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            links = soup.find_all('a')
            
            for link in links:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞
                    if link.get('href', '').startswith('http') and 'google.com' not in link.get('href', ''):
                        title = link.get_text().strip()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –ø—É—Å—Ç–æ–π –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
                        if len(title) > 10 and len(title) < 200:
                            url = link.get('href')
                            
                            # –ò—â–µ–º —Å–Ω–∏–ø–ø–µ—Ç —Ä—è–¥–æ–º —Å —Å—Å—ã–ª–∫–æ–π
                            snippet = ""
                            parent = link.parent
                            if parent:
                                text_elements = parent.find_all(text=True)
                                for text in text_elements:
                                    if text.strip() and text.strip() != title:
                                        snippet = text.strip()[:200]
                                        break
                            
                            results.append({
                                'title': title,
                                'url': url,
                                'snippet': snippet,
                                'source': 'Google'
                            })
                            
                            if len(results) >= 8:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                                break
                
                except Exception as e:
                    continue
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        
        return results
    
    def search_internet_multi(self, query: str) -> List[Dict]:
        """–ú—É–ª—å—Ç–∏–ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–≤–∏–∂–∫–æ–≤"""
        all_results = []
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –¥–≤–∏–∂–∫–∏
        search_methods = [
            ('DuckDuckGo', self.search_duckduckgo),
            ('Bing', self.search_bing),
            ('Google', self.search_google)
        ]
        
        for engine_name, search_func in search_methods:
            try:
                logger.info(f"–ü—Ä–æ–±—É–µ–º {engine_name}")
                results = search_func(query)
                if results:
                    all_results.extend(results)
                    logger.info(f"{engine_name} –¥–∞–ª {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                    if len(all_results) >= 8:
                        break
                time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ {engine_name}: {e}")
                continue
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_results = []
        seen_urls = set()
        
        for result in all_results:
            if result['url'] not in seen_urls:
                unique_results.append(result)
                seen_urls.add(result['url'])
        
        # –î–ª—è –ø–æ–≥–æ–¥–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –ø–æ–≥–æ–¥–Ω—ã–µ —Å–∞–π—Ç—ã
        if any(word in query.lower() for word in ['–ø–æ–≥–æ–¥–∞', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞', '–≥—Ä–∞–¥—É—Å']):
            weather_sites = [
                'gismeteo.ru', 'yandex.ru/pogoda', 'weather.com', 'accuweather.com', 'rp5.ru',
                'meteoinfo.ru', 'pogoda.ru', 'weather.yandex.ru', 'meteoweb.ru', 'meteo-tv.ru',
                'weather-forecast.com', 'worldweather.org', 'meteo.gov.ua', 'meteo.ua'
            ]
            weather_results = []
            other_results = []
            
            for result in unique_results:
                url = result['url'].lower()
                if any(site in url for site in weather_sites):
                    weather_results.append(result)
                else:
                    other_results.append(result)
            
            # –°–Ω–∞—á–∞–ª–∞ –ø–æ–≥–æ–¥–Ω—ã–µ —Å–∞–π—Ç—ã, –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
            unique_results = weather_results + other_results
        
        logger.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(unique_results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return unique_results[:8]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º 8 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    def search_weather(self, query: str) -> Optional[Dict]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ–≥–æ–¥—ã"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ—Ä–æ–¥ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        city_match = re.search(r'–ø–æ–≥–æ–¥[–∞–µ—É]?\s+(?:–≤\s+)?([–∞-—è—ë\s-]+)', query, re.IGNORECASE)
        if not city_match:
            city_match = re.search(r'—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä[–∞–µ—É]?\s+(?:–≤\s+)?([–∞-—è—ë\s-]+)', query, re.IGNORECASE)
        
        if city_match:
            city = city_match.group(1).strip()
        else:
            city = "–ú–æ—Å–∫–≤–∞"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–≥–∏–æ–Ω—ã
        city_fixes = {
            "—Ä–æ—Å—Ç–æ–≤–µ": "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É",
            "—Ä–æ—Å—Ç–æ–≤": "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É", 
            "—à–∞—Ö—Ç": "–®–∞—Ö—Ç—ã",
            "—à–∞—Ö—Ç–∞—Ö": "–®–∞—Ö—Ç—ã",
            "–º–æ—Å–∫–≤–µ": "–ú–æ—Å–∫–≤–∞",
            "–º–æ—Å–∫–≤–∞": "–ú–æ—Å–∫–≤–∞",
            "—Å–ø–±": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
            "–ø–∏—Ç–µ—Ä": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
            "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": "–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥",
            "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫",
            "–∫–∞–∑–∞–Ω—å": "–ö–∞–∑–∞–Ω—å",
            "–Ω–∏–∂–Ω–∏–π": "–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥",
            "—Å–∞–º–∞—Ä–∞": "–°–∞–º–∞—Ä–∞",
            "–æ–º—Å–∫": "–û–º—Å–∫",
            "—á–µ–ª—è–±–∏–Ω—Å–∫": "–ß–µ–ª—è–±–∏–Ω—Å–∫",
            "—É—Ñ–∞": "–£—Ñ–∞",
            "–≤–æ–ª–≥–æ–≥—Ä–∞–¥": "–í–æ–ª–≥–æ–≥—Ä–∞–¥",
            "–ø–µ—Ä–º—å": "–ü–µ—Ä–º—å",
            "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫": "–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫",
            "–≤–æ—Ä–æ–Ω–µ–∂": "–í–æ—Ä–æ–Ω–µ–∂",
            "—Å–∞—Ä–∞—Ç–æ–≤": "–°–∞—Ä–∞—Ç–æ–≤",
            "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä": "–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä",
            "—Ç–æ–ª—å—è—Ç—Ç–∏": "–¢–æ–ª—å—è—Ç—Ç–∏",
            "–∏–∂–µ–≤—Å–∫": "–ò–∂–µ–≤—Å–∫",
            "–±–∞—Ä–Ω–∞—É–ª": "–ë–∞—Ä–Ω–∞—É–ª",
            "—É–ª—å—è–Ω–æ–≤—Å–∫": "–£–ª—å—è–Ω–æ–≤—Å–∫",
            "–∏—Ä–∫—É—Ç—Å–∫": "–ò—Ä–∫—É—Ç—Å–∫",
            "—Ö–∞–±–∞—Ä–æ–≤—Å–∫": "–•–∞–±–∞—Ä–æ–≤—Å–∫",
            "—è—Ä–æ—Å–ª–∞–≤–ª—å": "–Ø—Ä–æ—Å–ª–∞–≤–ª—å",
            "–≤–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫": "–í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫",
            "–º–∞—Ö–∞—á–∫–∞–ª–∞": "–ú–∞—Ö–∞—á–∫–∞–ª–∞",
            "—Ç–æ–º—Å–∫": "–¢–æ–º—Å–∫",
            "–æ—Ä–µ–Ω–±—É—Ä–≥": "–û—Ä–µ–Ω–±—É—Ä–≥",
            "–∫–µ–º–µ—Ä–æ–≤–æ": "–ö–µ–º–µ—Ä–æ–≤–æ",
            "–Ω–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫": "–ù–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫",
            "—Ä—è–∑–∞–Ω—å": "–†—è–∑–∞–Ω—å",
            "–∞—Å—Ç—Ä–∞—Ö–∞–Ω—å": "–ê—Å—Ç—Ä–∞—Ö–∞–Ω—å",
            "–ø–µ–Ω–∑–∞": "–ü–µ–Ω–∑–∞",
            "–ª–∏–ø–µ—Ü–∫": "–õ–∏–ø–µ—Ü–∫",
            "–∫–∏—Ä–æ–≤": "–ö–∏—Ä–æ–≤",
            "—á–µ–±–æ–∫—Å–∞—Ä—ã": "–ß–µ–±–æ–∫—Å–∞—Ä—ã",
            "—Ç—É–ª–∞": "–¢—É–ª–∞",
            "–∫–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥": "–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥",
            "–∫—É—Ä—Å–∫": "–ö—É—Ä—Å–∫",
            "—É–ª–∞–Ω-—É–¥—ç": "–£–ª–∞–Ω-–£–¥—ç",
            "—Å—Ç–∞–≤—Ä–æ–ø–æ–ª—å": "–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å",
            "—Å–æ—á–∏": "–°–æ—á–∏",
            "–∏–≤–∞–Ω–æ–≤–æ": "–ò–≤–∞–Ω–æ–≤–æ",
            "–±—Ä—è–Ω—Å–∫": "–ë—Ä—è–Ω—Å–∫",
            "–±–µ–ª–≥–æ—Ä–æ–¥": "–ë–µ–ª–≥–æ—Ä–æ–¥",
            "–∞—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫": "–ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫",
            "–≤–ª–∞–¥–∏–º–∏—Ä": "–í–ª–∞–¥–∏–º–∏—Ä",
            "—Å–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å": "–°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å",
            "—á–∏—Ç–∞": "–ß–∏—Ç–∞",
            "–≥—Ä–æ–∑–Ω—ã–π": "–ì—Ä–æ–∑–Ω—ã–π",
            "–∫–∞–ª—É–≥–∞": "–ö–∞–ª—É–≥–∞",
            "—Å–º–æ–ª–µ–Ω—Å–∫": "–°–º–æ–ª–µ–Ω—Å–∫",
            "–≤–æ–ª–æ–≥–¥–∞": "–í–æ–ª–æ–≥–¥–∞",
            "–∫—É—Ä–≥–∞–Ω": "–ö—É—Ä–≥–∞–Ω",
            "–æ—Ä—ë–ª": "–û—Ä—ë–ª",
            "—Å–∞—Ä–∞–Ω—Å–∫": "–°–∞—Ä–∞–Ω—Å–∫",
            "–≤–ª–∞–¥–∏–∫–∞–≤–∫–∞–∑": "–í–ª–∞–¥–∏–∫–∞–≤–∫–∞–∑",
            "–º—É—Ä–º–∞–Ω—Å–∫": "–ú—É—Ä–º–∞–Ω—Å–∫",
            "—è–∫—É—Ç—Å–∫": "–Ø–∫—É—Ç—Å–∫",
            "–ø–µ—Ç—Ä–æ–∑–∞–≤–æ–¥—Å–∫": "–ü–µ—Ç—Ä–æ–∑–∞–≤–æ–¥—Å–∫",
            "—Å–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å": "–°–∏–º—Ñ–µ—Ä–æ–ø–æ–ª—å"
        }
        
        city_lower = city.lower()
        if city_lower in city_fixes:
            city = city_fixes[city_lower]
        
        logger.info(f"–ü–æ–∏—Å–∫ –ø–æ–≥–æ–¥—ã –¥–ª—è –≥–æ—Ä–æ–¥–∞: {city}")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            weather_queries = [
                f"–ø–æ–≥–æ–¥–∞ {city} —Å–µ–π—á–∞—Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥—Ä–∞–¥—É—Å—ã",
                f"–ø–æ–≥–æ–¥–∞ {city} —Å–µ–≥–æ–¥–Ω—è —Å–µ–π—á–∞—Å",
                f"{city} –ø–æ–≥–æ–¥–∞ —Å–µ–π—á–∞—Å –≥—Ä–∞–¥—É—Å—ã —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
                f"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤–æ–∑–¥—É—Ö–∞ {city} —Å–µ–π—á–∞—Å –≥—Ä–∞–¥—É—Å—ã",
                f"–ø–æ–≥–æ–¥–∞ {city} –†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
                f"{city} –ø–æ–≥–æ–¥–∞ —Å–µ–π—á–∞—Å —Ç–æ—á–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
                f"–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã {city} —Å–µ–π—á–∞—Å",
                f"–ø–æ–≥–æ–¥–∞ {city} –≥—Ä–∞–¥—É—Å—ã —Å–µ–π—á–∞—Å",
                f"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ {city} —Å–µ–π—á–∞—Å —Ç–æ—á–Ω–∞—è",
                f"–ø–æ–≥–æ–¥–∞ {city} —Å–µ–π—á–∞—Å –≥—Ä–∞–¥—É—Å—ã —Ü–µ–ª—å—Å–∏—è",
                f"{city} —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤–æ–∑–¥—É—Ö–∞ —Å–µ–π—á–∞—Å",
                f"–ø–æ–≥–æ–¥–∞ {city} —Å–µ–π—á–∞—Å –≤–ª–∞–∂–Ω–æ—Å—Ç—å –≤–µ—Ç–µ—Ä"
            ]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤
            if city == "–®–∞—Ö—Ç—ã":
                weather_queries.extend([
                    "–ø–æ–≥–æ–¥–∞ –®–∞—Ö—Ç—ã –†–æ—Å—Ç–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å —Å–µ–π—á–∞—Å",
                    "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –®–∞—Ö—Ç—ã —Å–µ–π—á–∞—Å –≥—Ä–∞–¥—É—Å—ã",
                    "–ø–æ–≥–æ–¥–∞ –®–∞—Ö—Ç—ã —Å–µ–π—á–∞—Å —Ç–æ—á–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
                    "–®–∞—Ö—Ç—ã –ø–æ–≥–æ–¥–∞ —Å–µ–π—á–∞—Å –≥—Ä–∞–¥—É—Å—ã —Ü–µ–ª—å—Å–∏—è"
                ])
            elif city == "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É":
                weather_queries.extend([
                    "–ø–æ–≥–æ–¥–∞ –†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É —Å–µ–π—á–∞—Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
                    "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É —Å–µ–π—á–∞—Å –≥—Ä–∞–¥—É—Å—ã",
                    "–ø–æ–≥–æ–¥–∞ –†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É —Å–µ–π—á–∞—Å —Ç–æ—á–Ω–∞—è",
                    "–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É –ø–æ–≥–æ–¥–∞ —Å–µ–π—á–∞—Å –≥—Ä–∞–¥—É—Å—ã"
                ])
            
            all_results = []
            for weather_query in weather_queries:
                results = self.search_internet_multi(weather_query)
                all_results.extend(results)
                if len(all_results) >= 8:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    break
            
            if all_results:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 6-8 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                weather_data = []
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–≥–æ–¥–Ω—ã–µ —Å–∞–π—Ç—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≥–æ—Ä–æ–¥–∞
                direct_weather_urls = [
                    f"https://www.gismeteo.ru/search/{city}/",
                    f"https://yandex.ru/pogoda/{city}",
                    f"https://pogoda.ru/{city}",
                    f"https://rp5.ru/–ü–æ–≥–æ–¥–∞_–≤_{city}"
                ]
                
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä—è–º—ã—Ö —Å—Å—ã–ª–æ–∫
                for url in direct_weather_urls:
                    try:
                        response = self.session.get(url, timeout=10)
                        if response.status_code == 200:
                            soup = BeautifulSoup(response.content, 'html.parser')
                            text = soup.get_text()
                            content = text[:1000] if len(text) > 1000 else text
                            
                            weather_data.append({
                                'title': f"–ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞: {url}",
                                'url': url,
                                'content': content,
                                'snippet': f"–ü—Ä—è–º—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–≥–æ–¥–µ –≤ {city}",
                                'source': 'Direct'
                            })
                            break  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–∏ {url}: {e}")
                        continue
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
                for result in all_results[:6]:
                    try:
                        response = self.session.get(result['url'], timeout=10)
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–µ–∑ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏)
                        text = soup.get_text()
                        
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞
                        content = text[:1000] if len(text) > 1000 else text
                        
                        weather_data.append({
                            'title': result['title'],
                            'url': result['url'],
                            'content': content,
                            'snippet': result.get('snippet', ''),
                            'source': result['source']
                        })
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
                        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        weather_data.append({
                            'title': result['title'],
                            'url': result['url'],
                            'content': result.get('snippet', ''),
                            'snippet': result.get('snippet', ''),
                            'source': result['source']
                        })
                
                return {
                    "city": city,
                    "sources": weather_data,
                    "summary": f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–≥–æ–¥–µ –≤ {city} –Ω–∞–π–¥–µ–Ω–∞ –≤ {len(weather_data)} –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"
                }
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ–≥–æ–¥—ã: {e}")
            return None
    
    def search_news(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º "–Ω–æ–≤–æ—Å—Ç–∏" –∫ –∑–∞–ø—Ä–æ—Å—É –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if "–Ω–æ–≤–æ—Å—Ç–∏" not in query.lower():
                query = f"–Ω–æ–≤–æ—Å—Ç–∏ {query}"
            
            logger.info(f"–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π: {query}")
            results = self.search_internet_multi(query)
            
            news_results = []
            for result in results[:8]:  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏
                    response = self.session.get(result['url'], timeout=10)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–±–µ–∑ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏)
                    text = soup.get_text()
                    
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 800 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞
                    content = text[:800] if len(text) > 800 else text
                    
                    news_results.append({
                        'title': result['title'],
                        'url': result['url'],
                        'content': content,
                        'snippet': result.get('snippet', ''),
                        'source': result['source']
                    })
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    news_results.append({
                        'title': result['title'],
                        'url': result['url'],
                        'content': result.get('snippet', ''),
                        'snippet': result.get('snippet', ''),
                        'source': result['source']
                    })
            
            return news_results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []
    
    def search_general(self, query: str) -> Dict:
        """–û–±—â–∏–π –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        try:
            logger.info(f"–û–±—â–∏–π –ø–æ–∏—Å–∫: {query}")
            results = self.search_internet_multi(query)
            
            if not results:
                return {
                    "found": False,
                    "summary": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                    "sources": []
                }
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            all_content = []
            sources = []
            
            for result in results[:8]:  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    response = self.session.get(result['url'], timeout=10)
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    text = soup.get_text()
                    
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 600 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞
                    content = text[:600] if len(text) > 600 else text
                    
                    all_content.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {result['title']}\n{content}")
                    sources.append(result['url'])
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–Ω–∏–ø–ø–µ—Ç
                    all_content.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {result['title']}\n{result.get('snippet', '')}")
                    sources.append(result['url'])
            
            combined_content = '\n\n'.join(all_content)
            
            return {
                "found": True,
                "content": combined_content,
                "sources": sources,
                "results_count": len(results)
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—â–µ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return {
                "found": False,
                "content": "–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
                "sources": []
            }
    
    def detect_query_type(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower()
        
        # –ü–æ–≥–æ–¥–∞
        if any(word in query_lower for word in ['–ø–æ–≥–æ–¥–∞', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞', '–≥—Ä–∞–¥—É—Å', '—Ö–æ–ª–æ–¥–Ω–æ', '–∂–∞—Ä–∫–æ']):
            return 'weather'
        
        # –ù–æ–≤–æ—Å—Ç–∏ - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        news_keywords = [
            '–Ω–æ–≤–æ—Å—Ç–∏', '–Ω–æ–≤–æ—Å—Ç—å', '–Ω–æ–≤–æ—Å—Ç–µ–π', '–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç', '—Å–ª—É—á–∏–ª–æ—Å—å', '—Å–æ–±—ã—Ç–∏–µ',
            '–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ', '—Å–µ–≥–æ–¥–Ω—è', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ', '—á—Ç–æ –Ω–æ–≤–æ–≥–æ', '—á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç',
            '–ø–æ—Å–ª–µ–¥–Ω–∏–µ', '—Å–≤–µ–∂–∏–µ', '–≥–ª–∞–≤–Ω—ã–µ'
        ]
        if any(word in query_lower for word in news_keywords):
            return 'news'
        
        # –û–±—â–∏–π –ø–æ–∏—Å–∫
        return 'general'
    
    def search_internet(self, query: str) -> Dict:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
        try:
            # –û—á–∏—â–∞–µ–º –∑–∞–ø—Ä–æ—Å
            clean_query = self.clean_query(query)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
            query_type = self.detect_query_type(clean_query)
            
            logger.info(f"–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {query_type}")
            
            if query_type == 'weather':
                result = self.search_weather(clean_query)
                if result:
                    return {
                        "success": True,
                        "type": "weather",
                        "data": result,
                        "query": clean_query
                    }
                else:
                    return {
                        "success": False,
                        "type": "weather",
                        "error": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–≥–æ–¥–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                        "query": clean_query
                    }
            
            elif query_type == 'news':
                results = self.search_news(clean_query)
                if results:
                    return {
                        "success": True,
                        "type": "news",
                        "data": results,
                        "query": clean_query
                    }
                else:
                    return {
                        "success": False,
                        "type": "news",
                        "error": "–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
                        "query": clean_query
                    }
            
            else:
                result = self.search_general(clean_query)
                return {
                    "success": result["found"],
                    "type": "general",
                    "data": result,
                    "query": clean_query
                }
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {e}")
            return {
                "success": False,
                "error": f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}",
                "query": query
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∏—Å—Ç–µ–º—ã
_internet_system = None

def get_internet_system() -> SimpleInternetSystem:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Å–∏—Å—Ç–µ–º—ã"""
    global _internet_system
    if _internet_system is None:
        _internet_system = SimpleInternetSystem()
    return _internet_system

def search_internet(query: str) -> Dict:
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
    system = get_internet_system()
    return system.search_internet(query)

# –¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è
if __name__ == "__main__":
    system = SimpleInternetSystem()
    
    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –ø–æ–≥–æ–¥—ã
    print("=== –¢–ï–°–¢ –ü–û–ò–°–ö–ê –ü–û–ì–û–î–´ ===")
    weather_result = system.search_internet("–±–æ—Ç, –∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ?")
    print(json.dumps(weather_result, indent=2, ensure_ascii=False))
    
    # –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    print("\n=== –¢–ï–°–¢ –ü–û–ò–°–ö–ê –ù–û–í–û–°–¢–ï–ô ===")
    news_result = system.search_internet("–±–æ—Ç, –∫–∞–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏?")
    print(json.dumps(news_result, indent=2, ensure_ascii=False))
    
    # –¢–µ—Å—Ç –æ–±—â–µ–≥–æ –ø–æ–∏—Å–∫–∞
    print("\n=== –¢–ï–°–¢ –û–ë–©–ï–ì–û –ü–û–ò–°–ö–ê ===")
    general_result = system.search_internet("–±–æ—Ç, —á—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?")
    print(json.dumps(general_result, indent=2, ensure_ascii=False)) 