#!/usr/bin/env python3
"""
üåê –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ò–ù–¢–ï–†–ù–ï–¢-–ò–ù–¢–ï–õ–õ–ï–ö–¢–ê –î–õ–Ø IKAR (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø)
–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –ø–æ–∏—Å–∫, –ø–∞—Ä—Å–∏–Ω–≥ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
"""

import asyncio
import aiohttp
import json
import logging
import re
import time
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
from urllib.parse import urlparse, quote_plus
from dataclasses import dataclass
import hashlib
import sqlite3
from pathlib import Path
import feedparser
from bs4 import BeautifulSoup
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    title: str
    url: str
    snippet: str
    source: str
    relevance_score: float
    timestamp: datetime
    content: Optional[str] = None
    processed_content: Optional[str] = None

@dataclass
class ProcessedInformation:
    """–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
    original_query: str
    search_results: List[SearchResult]
    ai_summary: str
    key_points: List[str]
    sources: List[str]
    confidence_score: float
    timestamp: datetime
    processing_time: float

class InternetIntelligenceSystem:
    """–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è)"""
    
    def __init__(self, openrouter_api_key: str = None):
        self.openrouter_api_key = openrouter_api_key or os.environ.get("OPENROUTER_API_KEY")
        self.session = None
        self.db_path = Path("data/internet_intelligence.db")
        self.db_path.parent.mkdir(exist_ok=True)
        self._init_database()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–≤–∏–∂–∫–æ–≤
        self.search_engines = {
            "google": {
                "enabled": True,
                "weight": 0.4,
                "max_results": 10
            },
            "bing": {
                "enabled": True,
                "weight": 0.3,
                "max_results": 8
            },
            "duckduckgo": {
                "enabled": True,
                "weight": 0.2,
                "max_results": 6
            },
            "news": {
                "enabled": True,
                "weight": 0.1,
                "max_results": 4
            }
        }
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
        self.news_sources = [
            "https://rss.cnn.com/rss/edition.rss",
            "https://feeds.bbci.co.uk/news/rss.xml",
            "https://www.reuters.com/tools/rss",
            "https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml",
            "https://feeds.feedburner.com/TechCrunch/",
            "https://www.wired.com/feed/rss",
            "https://feeds.arstechnica.com/arstechnica/index",
            "https://www.theverge.com/rss/index.xml"
        ]
        
        # –ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.cache = {}
        self.cache_duration = timedelta(hours=1)
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_cache (
                query_hash TEXT PRIMARY KEY,
                query_text TEXT,
                results TEXT,
                timestamp REAL,
                expires_at REAL
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query TEXT,
                results_count INTEGER,
                processing_time REAL,
                confidence_score REAL,
                timestamp REAL
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS information_sources (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE,
                title TEXT,
                domain TEXT,
                reliability_score REAL,
                last_accessed REAL,
                access_count INTEGER DEFAULT 0
            )
        ''')
        
        conn.commit()
        conn.close()
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
            )
        return self.session
    
    async def _search_google(self, query: str, max_results: int = 10) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Google (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É–±–ª–∏—á–Ω—ã–µ API)"""
        try:
            session = await self._get_session()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Google Custom Search API –∏–ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            search_url = f"https://www.google.com/search?q={quote_plus(query)}&num={max_results}"
            
            async with session.get(search_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    for result in soup.select('.g'):
                        try:
                            title_elem = result.select_one('h3')
                            link_elem = result.select_one('a')
                            snippet_elem = result.select_one('.VwiC3b')
                            
                            if title_elem and link_elem:
                                title = title_elem.get_text()
                                url = link_elem.get('href', '')
                                snippet = snippet_elem.get_text() if snippet_elem else ""
                                
                                # –û—á–∏—â–∞–µ–º URL
                                if url.startswith('/url?q='):
                                    url = url.split('/url?q=')[1].split('&')[0]
                                
                                results.append(SearchResult(
                                    title=title,
                                    url=url,
                                    snippet=snippet,
                                    source="google",
                                    relevance_score=0.8,
                                    timestamp=datetime.now()
                                ))
                        except Exception as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Google —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                    
                    return results[:max_results]
                else:
                    logger.warning(f"Google –ø–æ–∏—Å–∫ –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Google –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def _search_bing(self, query: str, max_results: int = 8) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Bing"""
        try:
            session = await self._get_session()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Bing Web Search API –∏–ª–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            search_url = f"https://www.bing.com/search?q={quote_plus(query)}&count={max_results}"
            
            async with session.get(search_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    for result in soup.select('.b_algo'):
                        try:
                            title_elem = result.select_one('h2 a')
                            snippet_elem = result.select_one('.b_caption p')
                            
                            if title_elem:
                                title = title_elem.get_text()
                                url = title_elem.get('href', '')
                                snippet = snippet_elem.get_text() if snippet_elem else ""
                                
                                results.append(SearchResult(
                                    title=title,
                                    url=url,
                                    snippet=snippet,
                                    source="bing",
                                    relevance_score=0.7,
                                    timestamp=datetime.now()
                                ))
                        except Exception as e:
                            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Bing —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                    
                    return results[:max_results]
                else:
                    logger.warning(f"Bing –ø–æ–∏—Å–∫ –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Bing –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def _search_duckduckgo(self, query: str, max_results: int = 6) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo"""
        try:
            session = await self._get_session()
            
            # DuckDuckGo Instant Answer API
            search_url = f"https://api.duckduckgo.com/?q={quote_plus(query)}&format=json&no_html=1&skip_disambig=1"
            
            async with session.get(search_url) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    results = []
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º Instant Answer –µ—Å–ª–∏ –µ—Å—Ç—å
                    if data.get('Abstract'):
                        results.append(SearchResult(
                            title=data.get('Heading', 'DuckDuckGo Answer'),
                            url=data.get('AbstractURL', ''),
                            snippet=data.get('Abstract', ''),
                            source="duckduckgo",
                            relevance_score=0.9,
                            timestamp=datetime.now()
                        ))
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º Related Topics
                    for topic in data.get('RelatedTopics', [])[:max_results-1]:
                        if isinstance(topic, dict) and topic.get('Text'):
                            results.append(SearchResult(
                                title=topic.get('FirstURL', '').split('/')[-1] or 'Related Topic',
                                url=topic.get('FirstURL', ''),
                                snippet=topic.get('Text', ''),
                                source="duckduckgo",
                                relevance_score=0.6,
                                timestamp=datetime.now()
                            ))
                    
                    return results[:max_results]
                else:
                    logger.warning(f"DuckDuckGo –ø–æ–∏—Å–∫ –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ DuckDuckGo –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def _search_news(self, query: str, max_results: int = 4) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ RSS"""
        try:
            session = await self._get_session()
            results = []
            
            for rss_url in self.news_sources[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                try:
                    async with session.get(rss_url) as response:
                        if response.status == 200:
                            content = await response.text()
                            feed = feedparser.parse(content)
                            
                            for entry in feed.entries[:max_results//len(self.news_sources[:3])]:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                                if self._is_relevant(entry.title + " " + entry.get('summary', ''), query):
                                    results.append(SearchResult(
                                        title=entry.title,
                                        url=entry.link,
                                        snippet=entry.get('summary', '')[:200],
                                        source="news",
                                        relevance_score=0.8,
                                        timestamp=datetime.now()
                                    ))
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ RSS {rss_url}: {e}")
            
            return results[:max_results]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []
    
    def _is_relevant(self, text: str, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å—É"""
        query_words = set(query.lower().split())
        text_words = set(text.lower().split())
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        common_words = query_words.intersection(text_words)
        return len(common_words) >= len(query_words) * 0.3
    
    async def search_internet(self, query: str, max_total_results: int = 20) -> List[SearchResult]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É"""
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫: '{query}'")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = hashlib.md5(query.encode()).hexdigest()
        cached_results = await self._get_cached_results(cache_key)
        if cached_results:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return cached_results
        
        all_results = []
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –¥–≤–∏–∂–∫–∞–º
        search_tasks = []
        
        if self.search_engines["google"]["enabled"]:
            search_tasks.append(self._search_google(query, self.search_engines["google"]["max_results"]))
        
        if self.search_engines["bing"]["enabled"]:
            search_tasks.append(self._search_bing(query, self.search_engines["bing"]["max_results"]))
        
        if self.search_engines["duckduckgo"]["enabled"]:
            search_tasks.append(self._search_duckduckgo(query, self.search_engines["duckduckgo"]["max_results"]))
        
        if self.search_engines["news"]["enabled"]:
            search_tasks.append(self._search_news(query, self.search_engines["news"]["max_results"]))
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for results in search_results:
            if isinstance(results, list):
                all_results.extend(results)
            else:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {results}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        unique_results = self._deduplicate_results(all_results)
        sorted_results = sorted(unique_results, key=lambda x: x.relevance_score, reverse=True)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        final_results = sorted_results[:max_total_results]
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await self._cache_results(cache_key, query, final_results)
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return final_results
    
    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            if result.url not in seen_urls:
                seen_urls.add(result.url)
                unique_results.append(result)
        
        return unique_results
    
    async def _get_cached_results(self, cache_key: str) -> Optional[List[SearchResult]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT results, expires_at FROM search_cache 
                WHERE query_hash = ? AND expires_at > ?
            ''', (cache_key, time.time()))
            
            row = cursor.fetchone()
            conn.close()
            
            if row:
                results_data = json.loads(row[0])
                return [SearchResult(**result) for result in results_data]
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫—ç—à–∞: {e}")
            return None
    
    async def _cache_results(self, cache_key: str, query: str, results: List[SearchResult]):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            results_data = [result.__dict__ for result in results]
            expires_at = time.time() + self.cache_duration.total_seconds()
            
            cursor.execute('''
                INSERT OR REPLACE INTO search_cache 
                (query_hash, query_text, results, timestamp, expires_at)
                VALUES (?, ?, ?, ?, ?)
            ''', (cache_key, query, json.dumps(results_data), time.time(), expires_at))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    
    async def extract_content(self, search_results: List[SearchResult]) -> List[SearchResult]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        logger.info(f"üìÑ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å {len(search_results)} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        async def extract_single(result: SearchResult) -> SearchResult:
            try:
                session = await self._get_session()
                
                async with session.get(result.url, timeout=10) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å BeautifulSoup
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                        for script in soup(["script", "style"]):
                            script.decompose()
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                        text = soup.get_text()
                        
                        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
                        lines = (line.strip() for line in text.splitlines())
                        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                        text = ' '.join(chunk for chunk in chunks if chunk)
                        
                        if text:
                            result.content = text[:3000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                        else:
                            result.content = result.snippet
                    else:
                        result.content = result.snippet
                        
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å {result.url}: {e}")
                result.content = result.snippet
            
            return result
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [extract_single(result) for result in search_results]
        results_with_content = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –æ—à–∏–±–∫–∏
        valid_results = []
        for result in results_with_content:
            if isinstance(result, SearchResult):
                valid_results.append(result)
            else:
                logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {result}")
        
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç —Å {len(valid_results)} —Å—Ç—Ä–∞–Ω–∏—Ü")
        return valid_results
    
    async def process_with_ai(self, query: str, search_results: List[SearchResult]) -> ProcessedInformation:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ AI"""
        logger.info("üß† –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —á–µ—Ä–µ–∑ AI")
        
        start_time = time.time()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è AI
        combined_content = f"–ó–∞–ø—Ä–æ—Å: {query}\n\n"
        
        for i, result in enumerate(search_results[:10], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            if result.content:
                combined_content += f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i} ({result.source}):\n"
                combined_content += f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {result.title}\n"
                combined_content += f"URL: {result.url}\n"
                combined_content += f"–ö–æ–Ω—Ç–µ–Ω—Ç: {result.content[:1000]}\n\n"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º AI-–≤—ã–∂–∏–º–∫—É
        ai_summary = await self._generate_ai_summary(query, combined_content)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
        key_points = await self._extract_key_points(ai_summary)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence_score = self._calculate_confidence(search_results, ai_summary)
        
        processing_time = time.time() - start_time
        
        return ProcessedInformation(
            original_query=query,
            search_results=search_results,
            ai_summary=ai_summary,
            key_points=key_points,
            sources=[result.url for result in search_results],
            confidence_score=confidence_score,
            timestamp=datetime.now(),
            processing_time=processing_time
        )
    
    async def _generate_ai_summary(self, query: str, content: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è AI-–≤—ã–∂–∏–º–∫–∏ —á–µ—Ä–µ–∑ OpenRouter"""
        try:
            if not self.openrouter_api_key:
                # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                return self._simple_summary(content)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenRouter –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã–∂–∏–º–∫–∏
            prompt = f"""
            –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}
            
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:
            {content[:4000]}
            
            –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫—É—é, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é –≤—ã–∂–∏–º–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. 
            –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã.
            """
            
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ OpenRouter API
            # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É
            return self._simple_summary(content)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ AI –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return self._simple_summary(content)
    
    def _simple_summary(self, content: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ AI"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', content)
        important_sentences = []
        
        keywords = ['–Ω–æ–≤–æ—Å—Ç–∏', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ', '–∏–∑–º–µ–Ω–µ–Ω–∏—è', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã', '–¥–∞–Ω–Ω—ã–µ', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞']
        
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in keywords):
                important_sentences.append(sentence.strip())
        
        if important_sentences:
            return '. '.join(important_sentences[:5]) + '.'
        else:
            return content[:500] + '...'
    
    async def _extract_key_points(self, summary: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤"""
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
        sentences = re.split(r'[.!?]+', summary)
        return [s.strip() for s in sentences if len(s.strip()) > 20][:5]
    
    def _calculate_confidence(self, results: List[SearchResult], summary: str) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö"""
        if not results:
            return 0.0
        
        # –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        avg_relevance = sum(r.relevance_score for r in results) / len(results)
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        source_diversity = min(len(set(r.source for r in results)) / 4, 1.0)
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_quality = sum(1 for r in results if r.content and len(r.content) > 100) / len(results)
        
        # –î–ª–∏–Ω–∞ –≤—ã–∂–∏–º–∫–∏
        summary_quality = min(len(summary) / 500, 1.0)
        
        confidence = (avg_relevance * 0.4 + source_diversity * 0.2 + 
                     content_quality * 0.2 + summary_quality * 0.2)
        
        return min(confidence, 1.0)
    
    async def get_internet_intelligence(self, query: str) -> ProcessedInformation:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"""
        logger.info(f"üåê –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        
        # 1. –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        search_results = await self.search_internet(query)
        
        if not search_results:
            logger.warning("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞")
            return ProcessedInformation(
                original_query=query,
                search_results=[],
                ai_summary="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.",
                key_points=[],
                sources=[],
                confidence_score=0.0,
                timestamp=datetime.now(),
                processing_time=0.0
            )
        
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        results_with_content = await self.extract_content(search_results)
        
        # 3. AI –æ–±—Ä–∞–±–æ—Ç–∫–∞
        processed_info = await self.process_with_ai(query, results_with_content)
        
        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        await self._save_to_history(processed_info)
        
        logger.info(f"‚úÖ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∑–∞–≤–µ—Ä—à–µ–Ω. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {processed_info.confidence_score:.2f}")
        
        return processed_info
    
    async def _save_to_history(self, processed_info: ProcessedInformation):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO search_history 
                (query, results_count, processing_time, confidence_score, timestamp)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                processed_info.original_query,
                len(processed_info.search_results),
                processed_info.processing_time,
                processed_info.confidence_score,
                time.time()
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é: {e}")
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–∏—Å—Ç–µ–º—ã"""
        if self.session and not self.session.closed:
            await self.session.close()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∏—Å—Ç–µ–º—ã
internet_system = None

async def get_internet_system() -> InternetIntelligenceSystem:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Å–∏—Å—Ç–µ–º—ã"""
    global internet_system
    if internet_system is None:
        internet_system = InternetIntelligenceSystem()
    return internet_system

async def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã"""
    system = await get_internet_system()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    query = "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞"
    
    print(f"üß† –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç: '{query}'")
    
    try:
        result = await system.get_internet_intelligence(query)
        
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"–ó–∞–ø—Ä–æ—Å: {result.original_query}")
        print(f"–ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(result.search_results)}")
        print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.processing_time:.2f}—Å")
        print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence_score:.2f}")
        
        print(f"\nüß† AI-–í–´–ñ–ò–ú–ö–ê:")
        print(result.ai_summary)
        
        print(f"\nüîë –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:")
        for i, point in enumerate(result.key_points, 1):
            print(f"{i}. {point}")
        
        print(f"\nüìö –ò–°–¢–û–ß–ù–ò–ö–ò:")
        for i, source in enumerate(result.sources[:5], 1):
            print(f"{i}. {source}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        await system.close()

if __name__ == "__main__":
    asyncio.run(main()) 