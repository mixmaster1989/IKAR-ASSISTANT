"""
–°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—ä–µ–∫—Ü–∏–∏ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ –ø—Ä–æ–º–ø—Ç—ã
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è DeepSeek —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º 160K —Ç–æ–∫–µ–Ω–æ–≤
"""

import asyncio
import json
import re
import sys
import os
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict
import tiktoken
import time

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ backend –≤ sys.path –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.collective_mind import get_collective_mind, CollectiveMemory
from utils.logger import get_logger

# –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–≥–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è
logger = get_logger('memory_injector')


@dataclass
class MemoryChunk:
    """–ß–∞–Ω–∫ –ø–∞–º—è—Ç–∏ –¥–ª—è –∏–Ω—ä–µ–∫—Ü–∏–∏ –≤ –ø—Ä–æ–º–ø—Ç"""
    id: str = ""
    content: str = ""
    relevance_score: float = 0.0
    memory_type: str = ""
    source_agent: str = ""
    agent_id: str = ""
    importance: float = 0.0
    tokens_count: int = 0
    context_tags: List[str] = None
    timestamp: float = 0.0
    success_rate: float = 0.0
    
    def __post_init__(self):
        if self.context_tags is None:
            self.context_tags = []


class MemoryInjector:
    """–°–∏—Å—Ç–µ–º–∞ –∏–Ω—ä–µ–∫—Ü–∏–∏ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ –ø—Ä–æ–º–ø—Ç—ã"""
    
    def __init__(self):
        self.collective_mind = get_collective_mind()
        self.tokenizer = tiktoken.encoding_for_model("gpt-4")  # –°–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è DeepSeek
        self.max_context_tokens = 160000  # 160K —Ç–æ–∫–µ–Ω–æ–≤
        self.memory_budget_ratio = 0.3    # 30% –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–∞–º—è—Ç–∏
        self.max_memory_tokens = int(self.max_context_tokens * self.memory_budget_ratio)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ç–∏–ø–æ–≤ –ø–∞–º—è—Ç–∏
        self.memory_type_weights = {
            'insight': 1.0,
            'wisdom': 0.9,
            'experience': 0.8,
            'observation': 0.7,
            'reflection': 0.6
        }
        
        # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        self.min_relevance_threshold = 0.5  # –ü–æ–Ω–∏–∂–µ–Ω —Å 0.7 –¥–æ 0.5 –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
        self.high_relevance_threshold = 0.8  # –î–ª—è –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
        
        # –ö–µ—à –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.relevance_cache = {}
        self.token_cache = {}
        
    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if text in self.token_cache:
            return self.token_cache[text]
        
        token_count = len(self.tokenizer.encode(text))
        self.token_cache[text] = token_count
        return token_count
    
    def extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '–æ', '–æ–±',
            '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–µ—Å–ª–∏', '—Ç–æ', '–∂–µ',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be'
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π –æ—Ç 3 —Å–∏–º–≤–æ–ª–æ–≤
        words = re.findall(r'\b\w{3,}\b', text.lower())
        keywords = [word for word in words if word not in stop_words]
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        return list(set(keywords))
    
    def calculate_relevance(self, memory: CollectiveMemory, query_keywords: List[str], 
                          context_keywords: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è"""
        cache_key = f"{memory.id}_{hash(tuple(query_keywords))}_{hash(tuple(context_keywords))}"
        
        if cache_key in self.relevance_cache:
            return self.relevance_cache[cache_key]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
        memory_keywords = self.extract_keywords(memory.content)
        memory_keywords.extend(getattr(memory, 'tags', []))
        
        # –†–∞—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –∑–∞–ø—Ä–æ—Å–æ–º
        query_matches = len(set(query_keywords) & set(memory_keywords))
        query_relevance = query_matches / max(len(query_keywords), 1)
        
        # –†–∞—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        context_matches = len(set(context_keywords) & set(memory_keywords))
        context_relevance = context_matches / max(len(context_keywords), 1)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–ª–∏–∑–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        semantic_score = 0.0
        for qword in query_keywords:
            for mword in memory_keywords:
                if qword in mword or mword in qword:
                    semantic_score += 0.1
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        relevance = (
            query_relevance * 0.5 +
            context_relevance * 0.3 +
            semantic_score * 0.2
        )
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ —Ç–∏–ø –ø–∞–º—è—Ç–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç—å
        type_bonus = self.memory_type_weights.get(memory.memory_type, 0.5)
        importance_bonus = memory.importance
        success_bonus = memory.success_rate
        
        final_relevance = relevance * (1 + type_bonus + importance_bonus + success_bonus)
        
        self.relevance_cache[cache_key] = final_relevance
        return final_relevance
    
    async def select_relevant_memories(self, query: str, context: str, user_id: str = None,
                                     max_memories: int = 10) -> List[MemoryChunk]:  # –ë—ã–ª–æ 50, —Ç–µ–ø–µ—Ä—å 10
        """–í—ã–±–æ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–ª–∞–¥–∫–∏ –ø–∞–º—è—Ç–∏
        try:
            import sys
            sys.path.append('backend')
            from utils.memory_debug_logger import get_memory_debug_logger
        except ImportError:
            # Fallback - —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
            class DummyLogger:
                def log_memory_injector_start(self, *args): pass
                def log_memory_injector_keywords(self, *args): pass
                def log_collective_wisdom_search(self, *args): pass
                def log_collective_wisdom_results(self, *args): pass
                def log_relevance_calculation(self, *args): pass
                def log_memory_chunks_selection(self, *args): pass
                def log_memory_chunk_details(self, *args): pass
                def log_no_memory_injection(self, *args): pass
                def log_memory_injection_result(self, *args): pass
                def log_error(self, *args): pass
            def get_memory_debug_logger():
                return DummyLogger()
        debug_logger = get_memory_debug_logger()
        debug_logger.log_memory_injector_start(query, context, user_id)
        
        if not self.collective_mind:
            debug_logger.log_no_memory_injection("collective_mind not available")
            return []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        query_keywords = self.extract_keywords(query)
        context_keywords = self.extract_keywords(context)
        debug_logger.log_memory_injector_keywords(query_keywords, context_keywords)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
        all_memories = []
        
        # üîí –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏, –ø–æ—Ç–æ–º –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ (–æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è)
        for keyword in query_keywords[:3]:  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 5 –¥–æ 3
            try:
                debug_logger.log_collective_wisdom_search(keyword, 10)
                memories = await self.collective_mind.get_collective_wisdom(
                    keyword, limit=10  # –£–º–µ–Ω—å—à–∏–ª–∏ —Å 20 –¥–æ 10
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ memories –Ω–µ None –∏ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º
                if memories and isinstance(memories, list):
                    debug_logger.log_collective_wisdom_results(keyword, len(memories), memories)
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ (—Å—Ç–∞—Ä—à–µ 90 –¥–Ω–µ–π)
                    current_time = time.time()
                    fresh_memories = [
                        memory for memory in memories 
                        if hasattr(memory, 'timestamp') and (current_time - memory.timestamp) < (90 * 24 * 60 * 60)  # 90 –¥–Ω–µ–π
                    ]
                    all_memories.extend(fresh_memories)
                else:
                    logger.debug(f"–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞: {keyword}")
                    
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –º—É–¥—Ä–æ—Å—Ç–∏ –¥–ª—è '{keyword}': {e}")
                continue
        
        # üîí –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∏—Å–∫ –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω user_id
        if user_id and len(all_memories) < 5:  # –ï—Å–ª–∏ –º–∞–ª–æ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
            try:
                from memory.lazy_memory import get_lazy_memory
                lazy_memory = get_lazy_memory()
                if lazy_memory:
                    personal_memories = lazy_memory.get_relevant_history(user_id, query, limit=10)
                    for memory in personal_memories:
                        # –°–æ–∑–¥–∞–µ–º MemoryChunk –∏–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏
                        chunk = MemoryChunk(
                            id=f"personal_{user_id}_{hash(memory['content'])}",  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
                            content=memory['content'],
                            relevance_score=0.8,  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏
                            memory_type='personal',
                            source_agent=user_id,
                            importance=0.9,
                            tokens_count=self.count_tokens(memory['content']),
                            context_tags=[],
                            timestamp=memory.get('timestamp', time.time())
                        )
                        all_memories.append(chunk)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è {user_id}: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_memories = {}
        for memory in all_memories:
            if memory.id not in unique_memories:
                unique_memories[memory.id] = memory
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
        memory_chunks = []
        for memory in unique_memories.values():
            relevance = self.calculate_relevance(memory, query_keywords, context_keywords)
            debug_logger.log_relevance_calculation(memory.id, relevance, self.min_relevance_threshold)
            
            if relevance > self.min_relevance_threshold:  # –ü–æ–≤—ã—à–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                chunk = MemoryChunk(
                    id=memory.id,
                    content=memory.content,
                    relevance_score=relevance,
                    memory_type=memory.memory_type,
                    source_agent=memory.agent_id,
                    importance=memory.importance,
                    tokens_count=self.count_tokens(memory.content),
                    context_tags=getattr(memory, 'tags', []),
                    timestamp=memory.timestamp
                )
                memory_chunks.append(chunk)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        memory_chunks.sort(key=lambda x: x.relevance_score, reverse=True)
        final_chunks = memory_chunks[:max_memories]
        
        debug_logger.log_memory_chunks_selection(len(memory_chunks), len(final_chunks), 0)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        for i, chunk in enumerate(final_chunks):
            debug_logger.log_memory_chunk_details(
                i, chunk.memory_type, chunk.relevance_score, 
                chunk.tokens_count, chunk.content
            )
        
        return final_chunks
    
    def optimize_memory_selection(self, memory_chunks: List[MemoryChunk], 
                                 available_tokens: int) -> List[MemoryChunk]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–±–æ—Ä–∞ –ø–∞–º—è—Ç–∏ –ø–æ–¥ –±—é–¥–∂–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤"""
        if not memory_chunks:
            return []
        
        # –ê–ª–≥–æ—Ä–∏—Ç–º —Ä—é–∫–∑–∞–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
        selected_chunks = []
        used_tokens = 0
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ —Ä–∞–∑–º–µ—Ä—É
        efficiency_sorted = sorted(
            memory_chunks, 
            key=lambda x: x.relevance_score / max(x.tokens_count, 1), 
            reverse=True
        )
        
        for chunk in efficiency_sorted:
            if used_tokens + chunk.tokens_count <= available_tokens:
                selected_chunks.append(chunk)
                used_tokens += chunk.tokens_count
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è —É—Ä–µ–∑–∞—Ç—å —á–∞–Ω–∫
                remaining_tokens = available_tokens - used_tokens
                if remaining_tokens > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                    truncated_content = self.truncate_content(
                        chunk.content, remaining_tokens
                    )
                    if truncated_content:
                        truncated_chunk = MemoryChunk(
                            content=truncated_content,
                            relevance_score=chunk.relevance_score * 0.8,  # –®—Ç—Ä–∞—Ñ –∑–∞ —É—Ä–µ–∑–∞–Ω–∏–µ
                            memory_type=chunk.memory_type,
                            source_agent=chunk.source_agent,
                            importance=chunk.importance,
                            tokens_count=remaining_tokens,
                            context_tags=chunk.context_tags,
                            timestamp=chunk.timestamp
                        )
                        selected_chunks.append(truncated_chunk)
                        used_tokens = available_tokens
                break
        
        return selected_chunks
    
    def truncate_content(self, content: str, max_tokens: int) -> str:
        """–£–º–Ω–æ–µ —É—Ä–µ–∑–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–º—ã—Å–ª–∞"""
        if self.count_tokens(content) <= max_tokens:
            return content
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', content)
        
        result = ""
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            test_content = result + sentence + ". "
            if self.count_tokens(test_content) <= max_tokens - 10:  # –ó–∞–ø–∞—Å
                result = test_content
            else:
                break
        
        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
        if len(result) < 50:
            tokens = self.tokenizer.encode(content)[:max_tokens-1]
            result = self.tokenizer.decode(tokens)
        
        return result.strip()
    
    def format_memory_injection(self, memory_chunks: List[MemoryChunk]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è –∏–Ω—ä–µ–∫—Ü–∏–∏ –≤ –ø—Ä–æ–º–ø—Ç"""
        if not memory_chunks:
            return ""
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –ø–∞–º—è—Ç–∏
        memory_groups = defaultdict(list)
        for chunk in memory_chunks:
            memory_groups[chunk.memory_type].append(chunk)
        
        formatted_sections = []
        
        for memory_type, chunks in memory_groups.items():
            if not chunks:
                continue
                
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏
            type_names = {
                'insight': '–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã',
                'wisdom': '–ú—É–¥—Ä–æ—Å—Ç—å —Å–µ—Ç–∏',
                'experience': '–û–ø—ã—Ç –∞–≥–µ–Ω—Ç–æ–≤',
                'observation': '–ù–∞–±–ª—é–¥–µ–Ω–∏—è',
                'reflection': '–†–∞–∑–º—ã—à–ª–µ–Ω–∏—è'
            }
            
            section_title = type_names.get(memory_type, '–ü–∞–º—è—Ç—å —Å–µ—Ç–∏')
            section_content = f"\n=== {section_title} ===\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
            for i, chunk in enumerate(chunks, 1):
                relevance_stars = "‚≠ê" * min(int(chunk.relevance_score * 5), 5)
                section_content += f"\n{i}. {relevance_stars} (–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {chunk.relevance_score:.2f})\n"
                section_content += f"   {chunk.content}\n"
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–∂–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
                if chunk.importance > 0.8:
                    section_content += f"   [–ò—Å—Ç–æ—á–Ω–∏–∫: {chunk.source_agent[:8]}..., –í–∞–∂–Ω–æ—Å—Ç—å: {chunk.importance:.2f}]\n"
            
            formatted_sections.append(section_content)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–µ–∫—Ü–∏–∏
        memory_injection = "\n".join(formatted_sections)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
        instruction = """
=== –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ ===
–í—ã—à–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏ —Å–µ—Ç–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ –∑–Ω–∞–Ω–∏—è –¥–ª—è:
- –û–±–æ–≥–∞—â–µ–Ω–∏—è —Å–≤–æ–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º –æ–ø—ã—Ç–æ–º
- –ò–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –¥—Ä—É–≥–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
- –ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è —É—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
- –°–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏—Ö –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É–∂–¥–µ–Ω–∏–π

–ü–æ–º–Ω–∏: –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å - —ç—Ç–æ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –∏—Å—Ç–∏–Ω–∞, –∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –æ–ø—ã—Ç.
–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–π –µ—ë –ø–æ–¥ —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
"""
        
        return instruction + memory_injection
    
    async def inject_memory_into_prompt(self, original_prompt: str, 
                                      context: str = "", 
                                      user_id: str = None,
                                      memory_budget_ratio: float = None) -> str:
        """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω—ä–µ–∫—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –≤ –ø—Ä–æ–º–ø—Ç"""
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–ª–∞–¥–∫–∏ –ø–∞–º—è—Ç–∏
        try:
            import sys
            sys.path.append('backend')
            from utils.memory_debug_logger import get_memory_debug_logger
        except ImportError:
            # Fallback - —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
            class DummyLogger:
                def log_memory_injector_start(self, *args): pass
                def log_memory_injector_keywords(self, *args): pass
                def log_collective_wisdom_search(self, *args): pass
                def log_collective_wisdom_results(self, *args): pass
                def log_relevance_calculation(self, *args): pass
                def log_memory_chunks_selection(self, *args): pass
                def log_memory_chunk_details(self, *args): pass
                def log_no_memory_injection(self, *args): pass
                def log_memory_injection_result(self, *args): pass
                def log_error(self, *args): pass
            def get_memory_debug_logger():
                return DummyLogger()
        debug_logger = get_memory_debug_logger()
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—é–¥–∂–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–∞–º—è—Ç–∏
            if memory_budget_ratio:
                memory_tokens = int(self.max_context_tokens * memory_budget_ratio)
            else:
                memory_tokens = self.max_memory_tokens
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
            original_tokens = self.count_tokens(original_prompt)
            context_tokens = self.count_tokens(context)
            
            # –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø–∞–º—è—Ç–∏
            available_tokens = memory_tokens - 500  # –ó–∞–ø–∞—Å –Ω–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            
            if available_tokens < 100:
                debug_logger.log_no_memory_injection(f"insufficient tokens: {available_tokens}")
                logger.warning("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∏–Ω—ä–µ–∫—Ü–∏–∏ –ø–∞–º—è—Ç–∏")
                return original_prompt
            
            # üîí –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–µ—Ä–µ–¥–∞–µ–º user_id –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
            memory_chunks = await self.select_relevant_memories(
                original_prompt, context, user_id
            )
            
            if not memory_chunks:
                debug_logger.log_no_memory_injection("no relevant memories found")
                logger.info("–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return original_prompt
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –≤—ã–±–æ—Ä –ø–æ–¥ –±—é–¥–∂–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
            selected_chunks = self.optimize_memory_selection(
                memory_chunks, available_tokens
            )
            
            if not selected_chunks:
                debug_logger.log_no_memory_injection("no suitable chunks after optimization")
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ–¥ –±—é–¥–∂–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤")
                return original_prompt
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å –¥–ª—è –∏–Ω—ä–µ–∫—Ü–∏–∏
            memory_injection = self.format_memory_injection(selected_chunks)
            
            # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
            enhanced_prompt = f"""{memory_injection}

=== –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å ===
{original_prompt}

=== –ö–æ–Ω—Ç–µ–∫—Å—Ç ===
{context}

–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä
            total_tokens = self.count_tokens(enhanced_prompt)
            original_tokens = self.count_tokens(original_prompt)
            injected_tokens = total_tokens - original_tokens
            
            debug_logger.log_memory_injection_result(original_tokens, injected_tokens, len(selected_chunks))
            
            logger.info(f"–ò–Ω—ä–µ–∫—Ü–∏—è –ø–∞–º—è—Ç–∏: {len(selected_chunks)} —á–∞–Ω–∫–æ–≤, "
                       f"{total_tokens} —Ç–æ–∫–µ–Ω–æ–≤, "
                       f"—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {selected_chunks[0].relevance_score:.2f}")
            
            return enhanced_prompt
            
        except Exception as e:
            debug_logger.log_error("memory_injector", e, {"prompt_length": len(original_prompt)})
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω—ä–µ–∫—Ü–∏–∏ –ø–∞–º—è—Ç–∏: {e}")
            return original_prompt
    
    async def analyze_memory_usage(self, prompt: str, user_id: str = None) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤ –ø—Ä–æ–º–ø—Ç–µ"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ collective_mind –¥–æ—Å—Ç—É–ø–µ–Ω
            if not self.collective_mind:
                logger.debug("collective_mind –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return {
                    'total_available': 0,
                    'relevance_distribution': {},
                    'type_distribution': {},
                    'token_usage': 0,
                    'error': 'collective_mind_not_available'
                }
            
            memory_chunks = await self.select_relevant_memories(prompt, "", user_id)
            
            if not memory_chunks:
                return {
                    'total_available': 0,
                    'relevance_distribution': {},
                    'type_distribution': {},
                    'token_usage': 0
                }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            relevance_ranges = {
                'high': len([c for c in memory_chunks if c.relevance_score > 0.7]),
                'medium': len([c for c in memory_chunks if 0.3 < c.relevance_score <= 0.7]),
                'low': len([c for c in memory_chunks if c.relevance_score <= 0.3])
            }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã –ø–∞–º—è—Ç–∏
            type_counts = defaultdict(int)
            for chunk in memory_chunks:
                type_counts[chunk.memory_type] += 1
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
            total_tokens = sum(chunk.tokens_count for chunk in memory_chunks)
            
            return {
                'total_available': len(memory_chunks),
                'relevance_distribution': relevance_ranges,
                'type_distribution': dict(type_counts),
                'token_usage': total_tokens,
                'top_relevance': memory_chunks[0].relevance_score if memory_chunks else 0,
                'memory_efficiency': len(memory_chunks) / max(total_tokens, 1) * 1000
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞–º—è—Ç–∏: {e}")
            return {
                'total_available': 0,
                'relevance_distribution': {},
                'type_distribution': {},
                'token_usage': 0,
                'error': str(e)
            }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∏–Ω–∂–µ–∫—Ç–æ—Ä–∞
memory_injector = None

def get_memory_injector() -> MemoryInjector:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∏–Ω–∂–µ–∫—Ç–æ—Ä–∞ –ø–∞–º—è—Ç–∏"""
    global memory_injector
    if memory_injector is None:
        memory_injector = MemoryInjector()
    return memory_injector


async def enhance_prompt_with_memory(prompt: str, context: str = "", 
                                   user_id: str = None,
                                   memory_budget: float = 0.3) -> str:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é"""
    injector = get_memory_injector()
    return await injector.inject_memory_into_prompt(prompt, context, user_id, memory_budget)


async def analyze_prompt_memory_potential(prompt: str) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
    injector = get_memory_injector()
    return await injector.analyze_memory_usage(prompt) 