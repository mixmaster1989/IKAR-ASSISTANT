"""
üîç Smart Retriever - –£–º–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫ –ø–∞–º—è—Ç–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º
–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import logging
import sqlite3
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import re

from .smart_memory_manager import SmartMemoryManager, MemoryChunk

logger = logging.getLogger("chatumba.smart_retriever")

@dataclass
class RelevantChunk:
    """–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —á–∞–Ω–∫ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    chunk: MemoryChunk
    relevance_score: float
    age_days: float
    threshold_passed: bool
    time_bonus: float
    context_bonus: float

class SmartRetriever:
    """–£–º–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫ –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, memory_manager: SmartMemoryManager):
        self.memory_manager = memory_manager
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞
        self.max_chunks = 5  # –ú–∞–∫—Å–∏–º—É–º —á–∞–Ω–∫–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
        self.base_relevance_weight = 0.6
        self.time_relevance_weight = 0.2
        self.context_relevance_weight = 0.2
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –±–æ–Ω—É—Å—ã
        self.time_of_day_bonus = {
            'morning': ['—É—Ç—Ä–æ', '—Ä–∞–±–æ—Ç–∞', '–ø–ª–∞–Ω—ã', '–≤—Å—Ç—Ä–µ—á–∞'],
            'afternoon': ['–æ–±–µ–¥', '—Ä–∞–±–æ—Ç–∞', '–ø—Ä–æ–µ–∫—Ç', '–∑–∞–¥–∞—á–∞'],
            'evening': ['–≤–µ—á–µ—Ä', '–æ—Ç–¥—ã—Ö', '–ø–ª–∞–Ω—ã', '–≤—Å—Ç—Ä–µ—á–∞'],
            'night': ['–Ω–æ—á—å', '–∑–∞–≤—Ç—Ä–∞', '–ø–æ–∑–¥–Ω–æ', '—Å–ø–∞—Ç—å']
        }
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.context_keywords = {
            'work': ['—Ä–∞–±–æ—Ç–∞', '–ø—Ä–æ–µ–∫—Ç', '–∑–∞–¥–∞—á–∞', '–≤—Å—Ç—Ä–µ—á–∞', '–¥–µ–¥–ª–∞–π–Ω', '–∫–æ–¥'],
            'personal': ['–¥–æ–º', '—Å–µ–º—å—è', '–¥—Ä—É–∑—å—è', '–æ—Ç–¥—ã—Ö', '—Ö–æ–±–±–∏'],
            'plans': ['–ø–ª–∞–Ω—ã', '–∑–∞–≤—Ç—Ä–∞', '–≤—Å—Ç—Ä–µ—á–∞', '—Å–æ–±—ã—Ç–∏–µ', '–Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ'],
            'problems': ['–ø—Ä–æ–±–ª–µ–º–∞', '–æ—à–∏–±–∫–∞', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–ø–æ–º–æ—â—å', '–≤–æ–ø—Ä–æ—Å']
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ –ø–∞–º—è—Ç–∏
        self.memory_query_keywords = [
            '—á—Ç–æ –ø–æ–º–Ω–∏—à—å', '—á—Ç–æ –∑–∞–ø–æ–º–Ω–∏–ª', '—á—Ç–æ –∑–Ω–∞–µ—à—å', '—Ä–∞—Å—Å–∫–∞–∂–∏ —á—Ç–æ –ø–æ–º–Ω–∏—à—å',
            '–ø–æ–∫–∞–∂–∏ –ø–∞–º—è—Ç—å', '—á—Ç–æ –≤ –ø–∞–º—è—Ç–∏', '–≤—Å—è –ø–∞–º—è—Ç—å', '–≤—Å–µ —á—Ç–æ –ø–æ–º–Ω–∏—à—å',
            '—á—Ç–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª', '—á—Ç–æ –∑–∞–ø–∏—Å–∞–ª', '–≤—Å–µ –∑–Ω–∞–Ω–∏—è', '–≤—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
            '–ø–∞–º—è—Ç—å', '–ü–ê–ú–Ø–¢–¨'  # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∏–≥–≥–µ—Ä —ç–∫—Å–ø–æ—Ä—Ç–∞ –ø–∞–º—è—Ç–∏
        ]
    
    async def find_relevant_chunks(self, chat_id: str, query: str, context: str = "") -> List[RelevantChunk]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –ø–∞–º—è—Ç–∏"""
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è —á–∞—Ç–∞ {chat_id}")
            logger.debug(f"Query: {query[:100]}...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å–æ–º –æ –ø–∞–º—è—Ç–∏
            is_memory_query = self._is_memory_query(query)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ —á–∞—Ç–∞
            all_chunks = await self._get_chat_chunks(chat_id)
            
            if not all_chunks:
                logger.info("üì≠ –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
                return []
            
            # –ï—Å–ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –æ –ø–∞–º—è—Ç–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
            if is_memory_query:
                logger.info(f"üß† –ó–∞–ø—Ä–æ—Å –æ –ø–∞–º—è—Ç–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
                relevant_chunks = []
                current_time = time.time()
                
                for chunk in all_chunks:
                    age_days = (current_time - chunk.created_at) / (24 * 3600)
                    relevant_chunk = RelevantChunk(
                        chunk=chunk,
                        relevance_score=1.0,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ –ø–∞–º—è—Ç–∏
                        age_days=age_days,
                        threshold_passed=True,
                        time_bonus=0.0,
                        context_bonus=0.0
                    )
                    relevant_chunks.append(relevant_chunk)
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
                relevant_chunks.sort(key=lambda x: x.chunk.created_at, reverse=True)
                return relevant_chunks
            
            # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
            relevant_chunks = []
            current_time = time.time()
            time_info = self.memory_manager.get_current_time_info()
            
            for chunk in all_chunks:
                # –í—ã—á–∏—Å–ª—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç —á–∞–Ω–∫–∞
                age_days = (current_time - chunk.created_at) / (24 * 3600)
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞
                threshold = self.memory_manager.calculate_relevance_threshold(age_days)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                relevance_score = await self._calculate_relevance(chunk, query, context, time_info)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–æ—à–µ–ª –ª–∏ —á–∞–Ω–∫ –ø–æ—Ä–æ–≥
                if relevance_score >= threshold:
                    # –í—ã—á–∏—Å–ª—è–µ–º –±–æ–Ω—É—Å—ã
                    time_bonus = self._calculate_time_bonus(chunk, time_info)
                    context_bonus = self._calculate_context_bonus(chunk, context)
                    
                    relevant_chunk = RelevantChunk(
                        chunk=chunk,
                        relevance_score=relevance_score,
                        age_days=age_days,
                        threshold_passed=True,
                        time_bonus=time_bonus,
                        context_bonus=context_bonus
                    )
                    relevant_chunks.append(relevant_chunk)
                else:
                    logger.debug(f"‚ùå –ß–∞–Ω–∫ {chunk.id} –Ω–µ –ø—Ä–æ—à–µ–ª –ø–æ—Ä–æ–≥: {relevance_score:.3f} < {threshold:.3f}")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Ç–æ–≥–æ–≤–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            relevant_chunks.sort(key=lambda x: x.relevance_score + x.time_bonus + x.context_bonus, reverse=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            result = relevant_chunks[:self.max_chunks]
            
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(result)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ {len(all_chunks)}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {e}")
            return []
    
    async def _get_chat_chunks(self, chat_id: str) -> List[MemoryChunk]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ —á–∞—Ç–∞"""
        try:
            conn = sqlite3.connect(self.memory_manager.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT id, chat_id, topic, content, created_at, source_period_start,
                       source_period_end, relevance_base, message_count, participants
                FROM memory_chunks
                WHERE chat_id = ?
                ORDER BY created_at DESC
            """, (chat_id,))
            
            chunks = []
            for row in cursor.fetchall():
                participants = json.loads(row[9]) if row[9] else []
                
                chunk = MemoryChunk(
                    id=row[0],
                    chat_id=row[1],
                    topic=row[2],
                    content=row[3],
                    created_at=row[4],
                    source_period_start=row[5],
                    source_period_end=row[6],
                    relevance_base=row[7],
                    message_count=row[8],
                    participants=participants
                )
                chunks.append(chunk)
            
            conn.close()
            return chunks
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —á–∞—Ç–∞: {e}")
            return []
    
    async def _calculate_relevance(self, chunk: MemoryChunk, query: str, context: str, time_info: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –±–∞–∑–æ–≤—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            query_keywords = self._extract_keywords(query.lower())
            context_keywords = self._extract_keywords(context.lower())
            
            # –¢–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            chunk_text = f"{chunk.topic} {chunk.content}".lower()
            
            # –ë–∞–∑–æ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keyword_score = 0.0
            total_keywords = len(query_keywords) + len(context_keywords)
            
            if total_keywords > 0:
                matches = 0
                for keyword in query_keywords + context_keywords:
                    if keyword in chunk_text:
                        matches += 1
                
                keyword_score = matches / total_keywords
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–ª–∏–∑–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
            semantic_score = self._calculate_semantic_similarity(query, chunk_text)
            
            # –ë–∞–∑–æ–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞
            importance_score = chunk.relevance_base
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            relevance = (
                keyword_score * self.base_relevance_weight +
                semantic_score * 0.3 +
                importance_score * 0.1
            )
            
            logger.debug(f"üìä –ß–∞–Ω–∫ {chunk.id}: keyword={keyword_score:.3f}, semantic={semantic_score:.3f}, importance={importance_score:.3f} ‚Üí {relevance:.3f}")
            
            return relevance
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
            return 0.0
    
    def _is_memory_query(self, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∑–∞–ø—Ä–æ—Å–æ–º –æ –ø–∞–º—è—Ç–∏"""
        query_lower = query.lower().strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        for keyword in self.memory_query_keywords:
            if keyword in query_lower:
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        memory_patterns = [
            r'–±–æ—Ç.*—á—Ç–æ.*–ø–æ–º–Ω–∏—à—å',
            r'–±–æ—Ç.*—á—Ç–æ.*–∑–∞–ø–æ–º–Ω–∏–ª',
            r'–±–æ—Ç.*—á—Ç–æ.*–∑–Ω–∞–µ—à—å',
            r'–±–æ—Ç.*—Ä–∞—Å—Å–∫–∞–∂–∏.*–ø–∞–º—è—Ç—å',
            r'–±–æ—Ç.*–ø–æ–∫–∞–∂–∏.*–ø–∞–º—è—Ç—å',
            r'–±–æ—Ç.*–≤—Å—è.*–ø–∞–º—è—Ç—å',
            r'–±–æ—Ç.*–≤—Å–µ.*–ø–æ–º–Ω–∏—à—å'
        ]
        
        for pattern in memory_patterns:
            if re.search(pattern, query_lower):
                return True
        
        return False
    
    async def get_all_group_chunks(self, chat_id: str) -> List[Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã"""
        try:
            logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø—ã {chat_id}")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –≥—Ä—É–ø–ø—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            all_chunks = await self._get_chat_chunks(chat_id)
            
            if not all_chunks:
                logger.info(f"üì≠ –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø—ã {chat_id}")
                return []
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø—ã {chat_id}")
            return all_chunks
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –≥—Ä—É–ø–ø—ã: {e}")
            return []
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '–æ', '–æ–±',
            '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–µ—Å–ª–∏', '—Ç–æ', '–∂–µ',
            '–∞', '–Ω–æ', '–∏–ª–∏', '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '–Ω–∏', '–±—ã', '–ª–∏', '—É–∂–µ', '–µ—â–µ'
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π –æ—Ç 3 —Å–∏–º–≤–æ–ª–æ–≤
        words = re.findall(r'\b\w{3,}\b', text)
        keywords = [word for word in words if word not in stop_words]
        
        return list(set(keywords))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    
    def _calculate_semantic_similarity(self, query: str, chunk_text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö —Å–ª–æ–≤
            query_words = set(self._extract_keywords(query.lower()))
            chunk_words = set(self._extract_keywords(chunk_text.lower()))
            
            if not query_words or not chunk_words:
                return 0.0
            
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞
            intersection = len(query_words & chunk_words)
            union = len(query_words | chunk_words)
            
            return intersection / union if union > 0 else 0.0
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏: {e}")
            return 0.0
    
    def _calculate_time_bonus(self, chunk: MemoryChunk, time_info: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –±–æ–Ω—É—Å"""
        try:
            time_of_day = time_info.get('time_of_day', 'day')
            bonus_keywords = self.time_of_day_bonus.get(time_of_day, [])
            
            chunk_text = f"{chunk.topic} {chunk.content}".lower()
            
            bonus = 0.0
            for keyword in bonus_keywords:
                if keyword in chunk_text:
                    bonus += 0.1
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –¥–ª—è –Ω–µ–¥–∞–≤–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ –≤ —Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è
            if time_info.get('hour', 12) in range(9, 18):  # –†–∞–±–æ—á–∏–µ —á–∞—Å—ã
                age_hours = (time.time() - chunk.created_at) / 3600
                if age_hours < 24:  # –ß–∞–Ω–∫–∏ –º–ª–∞–¥—à–µ —Å—É—Ç–æ–∫
                    bonus += 0.05
            
            return min(bonus, 0.3)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–æ–Ω—É—Å 0.3
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –±–æ–Ω—É—Å–∞: {e}")
            return 0.0
    
    def _calculate_context_bonus(self, chunk: MemoryChunk, context: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å"""
        try:
            if not context:
                return 0.0
            
            context_lower = context.lower()
            chunk_text = f"{chunk.topic} {chunk.content}".lower()
            
            bonus = 0.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            for category, keywords in self.context_keywords.items():
                context_matches = sum(1 for kw in keywords if kw in context_lower)
                chunk_matches = sum(1 for kw in keywords if kw in chunk_text)
                
                if context_matches > 0 and chunk_matches > 0:
                    bonus += 0.1 * min(context_matches, chunk_matches)
            
            return min(bonus, 0.2)  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–æ–Ω—É—Å 0.2
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –±–æ–Ω—É—Å–∞: {e}")
            return 0.0
    
    def format_chunks_for_prompt(self, relevant_chunks: List[RelevantChunk], time_info: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —á–∞–Ω–∫–∏ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç"""
        if not relevant_chunks:
            return ""
        
        current_time_str = time_info.get('datetime', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        
        formatted_parts = [
            f"=== –ü–ê–ú–Ø–¢–¨ –ì–†–£–ü–ü–´ ===",
            f"–¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: {current_time_str}",
            ""
        ]
        
        for i, rel_chunk in enumerate(relevant_chunks, 1):
            chunk = rel_chunk.chunk
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–∞
            chunk_time = datetime.fromtimestamp(chunk.created_at)
            age_str = self._format_age(rel_chunk.age_days)
            
            # –ü–µ—Ä–∏–æ–¥ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            period_start = datetime.fromtimestamp(chunk.source_period_start)
            period_end = datetime.fromtimestamp(chunk.source_period_end)
            
            formatted_parts.extend([
                f"{i}. üìù {chunk.topic} ({age_str})",
                f"   –ü–µ—Ä–∏–æ–¥: {period_start.strftime('%d.%m %H:%M')} - {period_end.strftime('%d.%m %H:%M')}",
                f"   –£—á–∞—Å—Ç–Ω–∏–∫–∏: {', '.join(chunk.participants[:3])}{'...' if len(chunk.participants) > 3 else ''}",
                f"   –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {chunk.content}",
                f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {rel_chunk.relevance_score:.2f} (–ø–æ—Ä–æ–≥ –ø—Ä–æ–π–¥–µ–Ω)",
                ""
            ])
        
        formatted_parts.extend([
            "=== –ò–ù–°–¢–†–£–ö–¶–ò–Ø ===",
            "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –ø–∞–º—è—Ç—å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É –¢–ï–ö–£–©–ï–ì–û —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.",
            "–ï—Å–ª–∏ –ø–∞–º—è—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ç–µ–∫—É—â–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏ - –∏–≥–Ω–æ—Ä–∏—Ä—É–π –ø–∞–º—è—Ç—å.",
            "–£—á–∏—Ç—ã–≤–∞–π –≤—Ä–µ–º—è: —Å—Ç–∞—Ä–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω–æ–π.",
            ""
        ])
        
        return "\n".join(formatted_parts)
    
    def _format_age(self, age_days: float) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç —á–∞–Ω–∫–∞"""
        if age_days < 1:
            hours = int(age_days * 24)
            return f"{hours}—á –Ω–∞–∑–∞–¥"
        elif age_days < 7:
            return f"{int(age_days)}–¥ –Ω–∞–∑–∞–¥"
        elif age_days < 30:
            weeks = int(age_days / 7)
            return f"{weeks}–Ω–µ–¥ –Ω–∞–∑–∞–¥"
        else:
            months = int(age_days / 30)
            return f"{months}–º–µ—Å –Ω–∞–∑–∞–¥"
    
    async def get_retriever_stats(self, chat_id: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞ –¥–ª—è —á–∞—Ç–∞"""
        try:
            chunks = await self._get_chat_chunks(chat_id)
            
            if not chunks:
                return {'total_chunks': 0}
            
            current_time = time.time()
            age_distribution = {'fresh': 0, 'recent': 0, 'old': 0, 'ancient': 0}
            
            for chunk in chunks:
                age_days = (current_time - chunk.created_at) / (24 * 3600)
                if age_days <= 7:
                    age_distribution['fresh'] += 1
                elif age_days <= 30:
                    age_distribution['recent'] += 1
                elif age_days <= 90:
                    age_distribution['old'] += 1
                else:
                    age_distribution['ancient'] += 1
            
            return {
                'total_chunks': len(chunks),
                'age_distribution': age_distribution,
                'oldest_chunk_days': max((current_time - chunk.created_at) / (24 * 3600) for chunk in chunks),
                'newest_chunk_days': min((current_time - chunk.created_at) / (24 * 3600) for chunk in chunks),
                'avg_relevance_base': sum(chunk.relevance_base for chunk in chunks) / len(chunks)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞: {e}")
            return {}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
_smart_retriever = None

def get_smart_retriever(memory_manager: SmartMemoryManager = None) -> SmartRetriever:
    """–ü–æ–ª—É—á–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞"""
    global _smart_retriever
    if _smart_retriever is None and memory_manager:
        _smart_retriever = SmartRetriever(memory_manager)
    return _smart_retriever